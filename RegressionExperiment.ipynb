{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "X_train, y_train = load_svmlight_file(\"a9a.txt\")[0], load_svmlight_file(\"a9a.txt\")[1]\n",
    "X_test, y_test = load_svmlight_file(\"a9a.t.txt\")[0], load_svmlight_file(\"a9a.t.txt\")[1]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "X_train = X_train.dot(np.eye(X_train.shape[1]))\n",
    "X_test = X_test.dot(np.eye(X_test.shape[1]))\n",
    "\n",
    "X_train = np.hstack((X_train,np.ones((X_train.shape[0],1)))) #把所有x的属性加上一个1，用于和w14（b）相乘\n",
    "\n",
    "X_test = np.hstack((X_test,np.ones((X_test.shape[0],1)))) #把所有x的属性加上一个0，（补齐数据文件的全零特征）\n",
    "X_test = np.hstack((X_test,np.ones((X_test.shape[0],1)))) #把所有x的属性加上一个1，用于和w14（b）相乘\n",
    "\n",
    "def changeLabel(y):#将label中的-1值全部改为0以便设计Loss函数\n",
    "    for i in range (y.shape[0]):\n",
    "        if y[i] == -1:\n",
    "            y[i] = 0\n",
    "\n",
    "changeLabel(y_train)\n",
    "changeLabel(y_test)\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "y_test = y_test.reshape((y_test.shape[0],1))\n",
    "\n",
    "\n",
    "w_sgd = np.zeros((X_train.shape[1],1)) #将SGD模型参数w初始化为全零\n",
    "w_nag = np.zeros((X_train.shape[1],1)) #将NAG模型参数w初始化为全零\n",
    "w_rmsp = np.zeros((X_train.shape[1],1)) #将RMSProp模型参数w初始化为全零\n",
    "w_adaD = np.zeros((X_train.shape[1],1)) #将AdaDelta模型参数w初始化为全零\n",
    "w_adam = np.zeros((X_train.shape[1],1)) #将Adam模型参数w初始化为全零\n",
    "\n",
    "n = 10000 #设置迭代次数\n",
    "\n",
    "LSGD = np.zeros((n)) #初始化用于保存LSGD的值随迭代次数变化的数组\n",
    "LNAG = np.zeros((n)) #初始化用于保存LNAG的值随迭代次数变化的数组\n",
    "LRMSProp = np.zeros((n)) #初始化用于保存LRMSProp的值随迭代次数变化的数组\n",
    "LAdaDelta = np.zeros((n)) #初始化用于保存LAdaDelta的值随迭代次数变化的数组\n",
    "LAdam = np.zeros((n)) #初始化用于保存LAdam的值随迭代次数变化的数组\n",
    "\n",
    "def Sigmod(w,X):#定义Sigmod函数,输出对每个样本都进行计算后的列向量\n",
    "    return 1/(1+np.exp(-((X.dot(w)))))\n",
    "\n",
    "def Lfun(w,X,y):#定义Loss函数\n",
    "    m = y.shape[0]\n",
    "    s = Sigmod(w,X)\n",
    "    l = -(y*np.log(s)+(1-y)*np.log(1-s))\n",
    "    return l.sum()\n",
    "\n",
    "def DER_SGD(w,X,y):#定义随机梯度下降的梯度函数\n",
    "    m = 100 #从数据集中随机取样\n",
    "    index = np.random.randint(0,X.shape[0],m)\n",
    "    X_sample = np.zeros((m,X.shape[1]))\n",
    "    y_sample = np.zeros((m,y.shape[1]))\n",
    " \n",
    "    for i in range (m):\n",
    "        X_sample[m-1] = X[index[m-1]]\n",
    "        y_sample[m-1] = y[index[m-1]]\n",
    "   \n",
    "    return -((X_sample.T).dot(y_sample-Sigmod(w,X_sample)))\n",
    "\n",
    "def cRate(w,X,y):#定义用于计算预测正确率的函数\n",
    "    m = y.shape[0]\n",
    "    j = (X.dot(w))\n",
    "    c = 0\n",
    "    for i in range (m):\n",
    "        if (j[i]>0 and y[i]==1) or (j[i]<0 and y[i]==0):\n",
    "            c = c + 1\n",
    "    return c/m\n",
    "\n",
    "def SGD(w,lnrt):\n",
    "    lnrt = lnrt\n",
    "    for i in range (n):#迭代若干次，更新模型参数w\n",
    "        g = (DER_SGD(w,X_train,y_train))\n",
    "        w -= lnrt*g\n",
    "        LSGD[i] = Lfun(w,X_test,y_test)\n",
    "\n",
    "def NAG(w,lnrt,u):\n",
    "    V = 0\n",
    "    u = u\n",
    "    lnrt = lnrt\n",
    "    for i in range (n):#迭代若干次，更新模型参数w\n",
    "        g = (DER_SGD(w,X_train,y_train))\n",
    "        V = u*V + lnrt*g\n",
    "        w -= V\n",
    "        LNAG[i] = Lfun(w,X_test,y_test)\n",
    "    \n",
    "def RMSProp(w,lnrt,u,e):\n",
    "    G = 0\n",
    "    u = u\n",
    "    e = e\n",
    "    lnrt = lnrt\n",
    "    for i in range (n):#迭代若干次，更新模型参数w\n",
    "        g = (DER_SGD(w,X_train,y_train))\n",
    "        G = u*G + (1-u)*g*g\n",
    "        w -=  (lnrt/((G+e)**0.5))*g\n",
    "        LRMSProp[i] = Lfun(w,X_test,y_test)\n",
    "        \n",
    "def AdaDelta(w,u,e):\n",
    "    G = 0\n",
    "    u = u\n",
    "    delta = 0\n",
    "    e = e\n",
    "    for i in range (n):#迭代若干次，更新模型参数w\n",
    "        g = (DER_SGD(w,X_train,y_train))\n",
    "        G = u*G + (1-u)*(g*g)\n",
    "        wdelta = -(((delta+e)**0.5)/((G+e)**0.5)) * g\n",
    "        w += wdelta\n",
    "        delta = u*delta + (1-u)*(wdelta*wdelta)\n",
    "        LAdaDelta[i] = Lfun(w,X_test,y_test)\n",
    "        \n",
    "def Adam(w,lnrt,u,e,B):\n",
    "    G = 0\n",
    "    u = u\n",
    "    B = B\n",
    "    m = 0\n",
    "    e = e\n",
    "    lnrt = lnrt\n",
    "    for i in range (n):#迭代若干次，更新模型参数w\n",
    "        g = (DER_SGD(w,X_train,y_train))\n",
    "        m = B*m + (1-B)*g\n",
    "        G = u*G + (1-u)*(g*g)\n",
    "        w -= (lnrt*((1-u**(i+1))**0.5)*m)/((1-B**(i+1))*((G+e)**0.5))\n",
    "        LAdam[i] = Lfun(w,X_test,y_test)\n",
    "\n",
    "\n",
    "SGD(w_sgd,0.01)\n",
    "NAG(w_nag,0.001,0.9)\n",
    "RMSProp(w_rmsp,0.0015,0.99,1e-7)\n",
    "AdaDelta(w_adaD,0.95,1e-6)\n",
    "Adam(w_adam,0.002,0.999,1e-8,0.9)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#绘制总图\n",
    "x = np.arange(0,n,1)\n",
    "plt.rcParams['figure.figsize']=(20,10)\n",
    "plt.plot(x, LSGD,  'black',label = 'LSGD')\n",
    "plt.plot(x, LNAG,  'r',label = 'LNAG')\n",
    "plt.plot(x, LRMSProp,  'b',label = 'LARMSProp')\n",
    "plt.plot(x, LAdaDelta,  'y',label = 'LAdaDelta')\n",
    "plt.plot(x, LAdam,  'g',label = 'LAdam')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Times of iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
